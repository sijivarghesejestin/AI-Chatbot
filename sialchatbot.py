{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Eq8EcBYjn1vlUtVJkBjtF_p5gQPlnGyV","authorship_tag":"ABX9TyN1uzUtazbsW0RmAe/jlzEN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\n","import random                                                                    #for choosing random responses\n","import json\n","import pickle                                                                    #for serialisation\n","import numpy as np\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","import tensorflow.keras\n","\n","                                                                    #for natural language processing\n","\n","import tensorflow.keras.optimizers\n","#import tensorflow as tf\n","#import tensorflow.python.keras as keras\n","#import keras.models\n","\n","#from keras.models import load_model\n","from nltk.stem import WordNetLemmatizer                             # recognise word with same stem(eg:work,worked,working etc)\n","from tensorflow.keras.models import Sequential                       #keras sequential which deals with ordering or sequencing of layers within a model.\n","from tensorflow.keras.layers import Dense, Activation, Dropout            #A dense layer is a classic fully connected neural network layer : each input node is connected to each output node. A dropout layer is similar except that when the layer is used, the activations are set to zero for some random nodes. This is a way to prevent overfitting.\n","from tensorflow.keras.optimizers import SGD     #Stochastic Gradient Descent (SGD) is a variant of the Gradient Descent algorithm that is used for optimizing machine learning models.\n","\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","\n","intents = json.loads(open('/content/drive/MyDrive/chatbot /intents.json').read())\n","\n","words = []\n","classes = []\n","documents = []\n","ignore_letters = ['?', '!', ',', '.']\n","\n","for intent in intents['intents']:\n","   for pattern in intent['patterns']:\n","       word_list = nltk.word_tokenize(pattern)                                      #splits a sentence into words\n","      # print(word_list)\n","       words.extend(word_list)                                                         #append new words to existing list\n","       documents.append((word_list, intent['tag']))                               #determines which word belong to which tag class\n","       #print(documents)\n","       if intent['tag'] not in classes:\n","           classes.append(intent['tag'])                                           # if words are not there in tag append to tag\n","\n","words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n","words = sorted(set(words))                                                              #to remove duplicate word use 'set'\n","#print(words)\n","classes = sorted(set(classes))\n","#print(classes)\n","# process to converts any kind of python objects (list, dict, etc.) into byte streams (0s and 1s) is called pickling or serialization or flattening or marshalling\n","#dump function takes 3 arguments. The first argument is the object that you want to store. The second argument is the file object you get by opening the desired file in write-binary (wb) mode. And the third argument is the key-value argument\n","\n","pickle.dump(words, open('words.pkl', 'wb'))                                                 #dump() writes the data to a file\n","pickle.dump(classes, open('classes.pkl', 'wb'))\n","\n","#neural network dont needs word it need numbers so we have to convert these chars/words to numbers so that it can work on\n","\n","\n","#below code appends all document data to training to work on neural network\n","training = []\n","output_empty = [0] * len(classes)\n","#print(output_empty)\n","\n","for document in documents:\n","    bag = []                                               # for each combination we will create a empty bag of words\n","    word_patterns = document[0]\n","\n","    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n","    #print(word_patterns)\n","    for word in words:\n","       bag.append(1) if word in word_patterns else bag.append(0)\n","\n","\n","    output_row = list(output_empty)                         #copy it to a list\n","    output_row[classes.index(document[1])] = 1              #set index to the output_row to 1\n","    training.append([bag, output_row])\n","    print(training)\n","\n","\n","\n","\n","    random.shuffle(training)                                #shuffle the training data\n","    #replace training instead of data\n","    training = np.array(training,dtype=\"object\")\n","\n","\n","    train_x = list(training[:, 0])\n","    train_y = list(training[:, 1])\n","\n","    # code to build neural network\n","\n","    model = Sequential()\n","    model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(len(train_y[0]), activation='softmax'))\n","\n","\n","    sgd = SGD(learning_rate= 0.01,weight_decay= 1e-6, momentum=0.9, nesterov=True)\n","    model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n","\n","    hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n","    model.save(\"chatbotmodel.h5\", hist)\n","    print(\"Done\")\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jHCBccpOXdIo","executionInfo":{"status":"error","timestamp":1697025781813,"user_tz":-330,"elapsed":6137,"user":{"displayName":"Siji","userId":"07036171848768678786"}},"outputId":"84b166e2-48ea-4636-e81a-6cfec05215d8"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]]\n","Epoch 1/200\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 686ms/step - loss: 1.9053 - accuracy: 0.0000e+00\n","Epoch 2/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.7801 - accuracy: 0.0000e+00\n","Epoch 3/200\n","1/1 [==============================] - 0s 13ms/step - loss: 1.8624 - accuracy: 0.0000e+00\n","Epoch 4/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.7276 - accuracy: 1.0000\n","Epoch 5/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.2610 - accuracy: 1.0000\n","Epoch 6/200\n","1/1 [==============================] - 0s 15ms/step - loss: 1.3994 - accuracy: 1.0000\n","Epoch 7/200\n","1/1 [==============================] - 0s 14ms/step - loss: 1.3060 - accuracy: 1.0000\n","Epoch 8/200\n","1/1 [==============================] - 0s 15ms/step - loss: 1.3894 - accuracy: 1.0000\n","Epoch 9/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.0143 - accuracy: 1.0000\n","Epoch 10/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.0805 - accuracy: 1.0000\n","Epoch 11/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.5801 - accuracy: 1.0000\n","Epoch 12/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6879 - accuracy: 1.0000\n","Epoch 13/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.3774 - accuracy: 1.0000\n","Epoch 14/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.3733 - accuracy: 1.0000\n","Epoch 15/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.4703 - accuracy: 1.0000\n","Epoch 16/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.2690 - accuracy: 1.0000\n","Epoch 17/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.2363 - accuracy: 1.0000\n","Epoch 18/200\n","1/1 [==============================] - 0s 19ms/step - loss: 0.1430 - accuracy: 1.0000\n","Epoch 19/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0454 - accuracy: 1.0000\n","Epoch 20/200\n","1/1 [==============================] - 0s 17ms/step - loss: 0.0550 - accuracy: 1.0000\n","Epoch 21/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0120 - accuracy: 1.0000\n","Epoch 22/200\n","1/1 [==============================] - 0s 17ms/step - loss: 0.0227 - accuracy: 1.0000\n","Epoch 23/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0340 - accuracy: 1.0000\n","Epoch 24/200\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0425 - accuracy: 1.0000\n","Epoch 25/200\n","1/1 [==============================] - 0s 18ms/step - loss: 0.0410 - accuracy: 1.0000\n","Epoch 26/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 27/200\n","1/1 [==============================] - 0s 19ms/step - loss: 0.0191 - accuracy: 1.0000\n","Epoch 28/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0040 - accuracy: 1.0000\n","Epoch 29/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0055 - accuracy: 1.0000\n","Epoch 30/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0153 - accuracy: 1.0000\n","Epoch 31/200\n","1/1 [==============================] - 0s 17ms/step - loss: 0.0018 - accuracy: 1.0000\n","Epoch 32/200\n","1/1 [==============================] - 0s 18ms/step - loss: 0.0037 - accuracy: 1.0000\n","Epoch 33/200\n","1/1 [==============================] - 0s 15ms/step - loss: 2.8761e-04 - accuracy: 1.0000\n","Epoch 34/200\n","1/1 [==============================] - 0s 21ms/step - loss: 5.9754e-04 - accuracy: 1.0000\n","Epoch 35/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0015 - accuracy: 1.0000\n","Epoch 36/200\n","1/1 [==============================] - 0s 14ms/step - loss: 7.9195e-04 - accuracy: 1.0000\n","Epoch 37/200\n","1/1 [==============================] - 0s 14ms/step - loss: 4.5015e-04 - accuracy: 1.0000\n","Epoch 38/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0018 - accuracy: 1.0000\n","Epoch 39/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0026 - accuracy: 1.0000\n","Epoch 40/200\n","1/1 [==============================] - 0s 15ms/step - loss: 1.4554e-04 - accuracy: 1.0000\n","Epoch 41/200\n","1/1 [==============================] - 0s 14ms/step - loss: 5.3332e-04 - accuracy: 1.0000\n","Epoch 42/200\n","1/1 [==============================] - 0s 14ms/step - loss: 3.6376e-04 - accuracy: 1.0000\n","Epoch 43/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 44/200\n","1/1 [==============================] - 0s 17ms/step - loss: 0.0067 - accuracy: 1.0000\n","Epoch 45/200\n","1/1 [==============================] - 0s 19ms/step - loss: 0.0532 - accuracy: 1.0000\n","Epoch 46/200\n","1/1 [==============================] - 0s 17ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n","Epoch 47/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000\n","Epoch 48/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0093 - accuracy: 1.0000\n","Epoch 49/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0015 - accuracy: 1.0000\n","Epoch 50/200\n","1/1 [==============================] - 0s 14ms/step - loss: 1.2659e-04 - accuracy: 1.0000\n","Epoch 51/200\n","1/1 [==============================] - 0s 13ms/step - loss: 1.3339e-04 - accuracy: 1.0000\n","Epoch 52/200\n","1/1 [==============================] - 0s 13ms/step - loss: 5.5655e-04 - accuracy: 1.0000\n","Epoch 53/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0064 - accuracy: 1.0000\n","Epoch 54/200\n","1/1 [==============================] - 0s 11ms/step - loss: 8.0105e-05 - accuracy: 1.0000\n","Epoch 55/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 56/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.7975e-04 - accuracy: 1.0000\n","Epoch 57/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0093 - accuracy: 1.0000\n","Epoch 58/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0038 - accuracy: 1.0000\n","Epoch 59/200\n","1/1 [==============================] - 0s 12ms/step - loss: 6.9153e-04 - accuracy: 1.0000\n","Epoch 60/200\n","1/1 [==============================] - 0s 12ms/step - loss: 7.8600e-04 - accuracy: 1.0000\n","Epoch 61/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0134 - accuracy: 1.0000\n","Epoch 62/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0033 - accuracy: 1.0000\n","Epoch 63/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.3192e-04 - accuracy: 1.0000\n","Epoch 64/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.3529e-04 - accuracy: 1.0000\n","Epoch 65/200\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0036 - accuracy: 1.0000\n","Epoch 66/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0398 - accuracy: 1.0000\n","Epoch 67/200\n","1/1 [==============================] - 0s 13ms/step - loss: 8.2030e-04 - accuracy: 1.0000\n","Epoch 68/200\n","1/1 [==============================] - 0s 12ms/step - loss: 7.8079e-05 - accuracy: 1.0000\n","Epoch 69/200\n","1/1 [==============================] - 0s 11ms/step - loss: 8.8688e-05 - accuracy: 1.0000\n","Epoch 70/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0229 - accuracy: 1.0000\n","Epoch 71/200\n","1/1 [==============================] - 0s 11ms/step - loss: 2.6941e-05 - accuracy: 1.0000\n","Epoch 72/200\n","1/1 [==============================] - 0s 11ms/step - loss: 6.5996e-04 - accuracy: 1.0000\n","Epoch 73/200\n","1/1 [==============================] - 0s 11ms/step - loss: 6.1272e-05 - accuracy: 1.0000\n","Epoch 74/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - accuracy: 1.0000\n","Epoch 75/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0041 - accuracy: 1.0000\n","Epoch 76/200\n","1/1 [==============================] - 0s 11ms/step - loss: 2.0013e-04 - accuracy: 1.0000\n","Epoch 77/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.2730e-04 - accuracy: 1.0000\n","Epoch 78/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0254 - accuracy: 1.0000\n","Epoch 79/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.2278e-05 - accuracy: 1.0000\n","Epoch 80/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 81/200\n","1/1 [==============================] - 0s 11ms/step - loss: 5.3404e-05 - accuracy: 1.0000\n","Epoch 82/200\n","1/1 [==============================] - 0s 13ms/step - loss: 1.7881e-06 - accuracy: 1.0000\n","Epoch 83/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.4345e-05 - accuracy: 1.0000\n","Epoch 84/200\n","1/1 [==============================] - 0s 11ms/step - loss: 5.7299e-04 - accuracy: 1.0000\n","Epoch 85/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.1126e-05 - accuracy: 1.0000\n","Epoch 86/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.8755e-05 - accuracy: 1.0000\n","Epoch 87/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.7881e-06 - accuracy: 1.0000\n","Epoch 88/200\n","1/1 [==============================] - 0s 11ms/step - loss: 2.1896e-04 - accuracy: 1.0000\n","Epoch 89/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 90/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0148 - accuracy: 1.0000\n","Epoch 91/200\n","1/1 [==============================] - 0s 11ms/step - loss: 2.1458e-06 - accuracy: 1.0000\n","Epoch 92/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.2544e-05 - accuracy: 1.0000\n","Epoch 93/200\n","1/1 [==============================] - 0s 11ms/step - loss: 2.6425e-04 - accuracy: 1.0000\n","Epoch 94/200\n","1/1 [==============================] - 0s 11ms/step - loss: 5.4416e-04 - accuracy: 1.0000\n","Epoch 95/200\n","1/1 [==============================] - 0s 11ms/step - loss: 6.5205e-05 - accuracy: 1.0000\n","Epoch 96/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.5179e-05 - accuracy: 1.0000\n","Epoch 97/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.0204e-04 - accuracy: 1.0000\n","Epoch 98/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0926 - accuracy: 1.0000\n","Epoch 99/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0066 - accuracy: 1.0000\n","Epoch 100/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - accuracy: 1.0000\n","Epoch 101/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.7486e-04 - accuracy: 1.0000\n","Epoch 102/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.3842e-06 - accuracy: 1.0000\n","Epoch 103/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.4424e-05 - accuracy: 1.0000\n","Epoch 104/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0629 - accuracy: 1.0000\n","Epoch 105/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0019 - accuracy: 1.0000\n","Epoch 106/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.2016e-04 - accuracy: 1.0000\n","Epoch 107/200\n","1/1 [==============================] - 0s 10ms/step - loss: 3.2424e-05 - accuracy: 1.0000\n","Epoch 108/200\n","1/1 [==============================] - 0s 11ms/step - loss: 8.4993e-05 - accuracy: 1.0000\n","Epoch 109/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.0549e-04 - accuracy: 1.0000\n","Epoch 110/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.6492e-06 - accuracy: 1.0000\n","Epoch 111/200\n","1/1 [==============================] - 0s 11ms/step - loss: 6.7949e-06 - accuracy: 1.0000\n","Epoch 112/200\n","1/1 [==============================] - 0s 11ms/step - loss: 8.7022e-06 - accuracy: 1.0000\n","Epoch 113/200\n","1/1 [==============================] - 0s 15ms/step - loss: 9.3178e-04 - accuracy: 1.0000\n","Epoch 114/200\n","1/1 [==============================] - 0s 11ms/step - loss: 8.1041e-04 - accuracy: 1.0000\n","Epoch 115/200\n","1/1 [==============================] - 0s 11ms/step - loss: 8.5830e-06 - accuracy: 1.0000\n","Epoch 116/200\n","1/1 [==============================] - 0s 12ms/step - loss: 2.0265e-05 - accuracy: 1.0000\n","Epoch 117/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0039 - accuracy: 1.0000\n","Epoch 118/200\n","1/1 [==============================] - 0s 11ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n","Epoch 119/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.0636e-05 - accuracy: 1.0000\n","Epoch 120/200\n","1/1 [==============================] - 0s 11ms/step - loss: 2.9214e-04 - accuracy: 1.0000\n","Epoch 121/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.0133e-05 - accuracy: 1.0000\n","Epoch 122/200\n","1/1 [==============================] - 0s 12ms/step - loss: 5.5012e-04 - accuracy: 1.0000\n","Epoch 123/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.8517e-05 - accuracy: 1.0000\n","Epoch 124/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0086 - accuracy: 1.0000\n","Epoch 125/200\n","1/1 [==============================] - 0s 11ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n","Epoch 126/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 127/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.7881e-06 - accuracy: 1.0000\n","Epoch 128/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0038 - accuracy: 1.0000\n","Epoch 129/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.6954e-05 - accuracy: 1.0000\n","Epoch 130/200\n","1/1 [==============================] - 0s 11ms/step - loss: 6.4609e-05 - accuracy: 1.0000\n","Epoch 131/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 132/200\n","1/1 [==============================] - 0s 18ms/step - loss: 1.7701e-04 - accuracy: 1.0000\n","Epoch 133/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0012 - accuracy: 1.0000\n","Epoch 134/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.9073e-05 - accuracy: 1.0000\n","Epoch 135/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 136/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.5244e-04 - accuracy: 1.0000\n","Epoch 137/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.0729e-06 - accuracy: 1.0000\n","Epoch 138/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.5034e-06 - accuracy: 1.0000\n","Epoch 139/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0301 - accuracy: 1.0000\n","Epoch 140/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0040 - accuracy: 1.0000\n","Epoch 141/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 142/200\n","1/1 [==============================] - 0s 11ms/step - loss: 5.0615e-04 - accuracy: 1.0000\n","Epoch 143/200\n","1/1 [==============================] - 0s 11ms/step - loss: 8.4039e-05 - accuracy: 1.0000\n","Epoch 144/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0611 - accuracy: 1.0000\n","Epoch 145/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.9072e-04 - accuracy: 1.0000\n","Epoch 146/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 147/200\n","1/1 [==============================] - 0s 15ms/step - loss: 1.2027e-04 - accuracy: 1.0000\n","Epoch 148/200\n","1/1 [==============================] - 0s 13ms/step - loss: 2.3842e-05 - accuracy: 1.0000\n","Epoch 149/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0019 - accuracy: 1.0000\n","Epoch 150/200\n","1/1 [==============================] - 0s 13ms/step - loss: 6.0676e-05 - accuracy: 1.0000\n","Epoch 151/200\n","1/1 [==============================] - 0s 12ms/step - loss: 7.0333e-06 - accuracy: 1.0000\n","Epoch 152/200\n","1/1 [==============================] - 0s 10ms/step - loss: 4.8636e-05 - accuracy: 1.0000\n","Epoch 153/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0024 - accuracy: 1.0000\n","Epoch 154/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 155/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.7200e-04 - accuracy: 1.0000\n","Epoch 156/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.7466e-04 - accuracy: 1.0000\n","Epoch 157/200\n","1/1 [==============================] - 0s 10ms/step - loss: 8.1062e-06 - accuracy: 1.0000\n","Epoch 158/200\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0167 - accuracy: 1.0000\n","Epoch 159/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.0967e-05 - accuracy: 1.0000\n","Epoch 160/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.7665e-04 - accuracy: 1.0000\n","Epoch 161/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0032 - accuracy: 1.0000\n","Epoch 162/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - accuracy: 1.0000\n","Epoch 163/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.7684e-06 - accuracy: 1.0000\n","Epoch 164/200\n","1/1 [==============================] - 0s 11ms/step - loss: 7.1464e-04 - accuracy: 1.0000\n","Epoch 165/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.7881e-06 - accuracy: 1.0000\n","Epoch 166/200\n","1/1 [==============================] - 0s 10ms/step - loss: 3.7115e-04 - accuracy: 1.0000\n","Epoch 167/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.5497e-05 - accuracy: 1.0000\n","Epoch 168/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.2671e-04 - accuracy: 1.0000\n","Epoch 169/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0364 - accuracy: 1.0000\n","Epoch 170/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.1831e-04 - accuracy: 1.0000\n","Epoch 171/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.6028e-06 - accuracy: 1.0000\n","Epoch 172/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0048 - accuracy: 1.0000\n","Epoch 173/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.3029e-04 - accuracy: 1.0000\n","Epoch 174/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.9683e-05 - accuracy: 1.0000\n","Epoch 175/200\n","1/1 [==============================] - 0s 11ms/step - loss: 7.7486e-06 - accuracy: 1.0000\n","Epoch 176/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.6226e-06 - accuracy: 1.0000\n","Epoch 177/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0058 - accuracy: 1.0000\n","Epoch 178/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0048 - accuracy: 1.0000\n","Epoch 179/200\n","1/1 [==============================] - 0s 13ms/step - loss: 1.5413e-04 - accuracy: 1.0000\n","Epoch 180/200\n","1/1 [==============================] - 0s 12ms/step - loss: 2.1457e-05 - accuracy: 1.0000\n","Epoch 181/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0098 - accuracy: 1.0000\n","Epoch 182/200\n","1/1 [==============================] - 0s 10ms/step - loss: 9.3262e-04 - accuracy: 1.0000\n","Epoch 183/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.4471e-04 - accuracy: 1.0000\n","Epoch 184/200\n","1/1 [==============================] - 0s 10ms/step - loss: 1.0133e-05 - accuracy: 1.0000\n","Epoch 185/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 186/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.7656e-05 - accuracy: 1.0000\n","Epoch 187/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0051 - accuracy: 1.0000\n","Epoch 188/200\n","1/1 [==============================] - 0s 10ms/step - loss: 3.5763e-07 - accuracy: 1.0000\n","Epoch 189/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0045 - accuracy: 1.0000\n","Epoch 190/200\n","1/1 [==============================] - 0s 10ms/step - loss: 6.6874e-05 - accuracy: 1.0000\n","Epoch 191/200\n","1/1 [==============================] - 0s 10ms/step - loss: 6.7470e-05 - accuracy: 1.0000\n","Epoch 192/200\n","1/1 [==============================] - 0s 10ms/step - loss: 3.3020e-05 - accuracy: 1.0000\n","Epoch 193/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 194/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.5901e-04 - accuracy: 1.0000\n","Epoch 195/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.9908e-05 - accuracy: 1.0000\n","Epoch 196/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0087 - accuracy: 1.0000\n","Epoch 197/200\n","1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 198/200\n","1/1 [==============================] - 0s 13ms/step - loss: 6.6636e-05 - accuracy: 1.0000\n","Epoch 199/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n","Epoch 200/200\n","1/1 [==============================] - 0s 13ms/step - loss: 2.5436e-04 - accuracy: 1.0000\n","Done\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-f936a897bd70>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0moutput_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_empty\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;31m#copy it to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0moutput_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m              \u001b[0;31m#set index to the output_row to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_row\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"]}]},{"cell_type":"code","source":["import random\n","import json\n","import pickle\n","import numpy as np\n","\n","import nltk\n","from nltk.stem import  WordNetLemmatizer\n","\n","\n","from tensorflow.keras.models import load_model\n","\n","lemmatizer = WordNetLemmatizer()\n","intents = json.loads(open('/content/drive/MyDrive/chatbot /intents.json').read())\n","\n","words = pickle.load(open('words.pkl', 'rb'))\n","classes = pickle.load(open('classes.pkl', 'rb'))\n","model = load_model('chatbotmodel.h5')\n","#print(model)\n","\n","\n","#function for cleaning up sentence\n","\n","def clean_up_sentence(sentence):\n","    sentence_words = nltk.word_tokenize(sentence)\n","    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n","    return sentence_words\n","    #print(sentence_words)\n","\n","# function to check if word is present\n","\n","def bag_of_words(sentence):\n","    sentence_words = clean_up_sentence(sentence)\n","    words = pickle.load(open('words.pkl', 'rb'))\n","    #print(words)\n","    bag = [0] * len(words)\n","    for w in sentence_words:\n","         for i, word in enumerate(words):\n","             if word == w:\n","                 bag[i] = 1\n","             return np.array(bag)\n","\n","# function for providing response in chatbot\n","\n","def predict_class(sentence):\n","\n","     bow = bag_of_words(sentence)\n","     #print(bow)\n","     res = model.predict(np.array([bow]))[0]\n","     #print(res)\n","     ERROR_THRESHOLD = 0.5\n","     results = [[i, r] for i,r in enumerate(res) if r > ERROR_THRESHOLD]\n","\n","\n","     results.sort(key=lambda x: x[1], reverse = True)\n","     return_list=[]\n","     for r in results:\n","         return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n","         return return_list\n","         #print(return_list)\n","\n","def  get_response(intents_list, intents_json):\n","     tag = intents_list[0]['intent']\n","     list_of_intents = intents_json['intents']\n","     for i in list_of_intents:\n","         if i['tag'] == tag:\n","             result = random.choice(i['responses'])\n","             break\n","     return result\n","\n","print(\"Go bot is running\")\n","\n","while True:\n","\n","    message = input(\"\")\n","    ints =  predict_class(message)\n","    res = get_response(ints, intents)\n","    print(res)\n","\n","\n","\n","\n"],"metadata":{"id":"DraaFBiYo1Af","colab":{"base_uri":"https://localhost:8080/","height":529},"executionInfo":{"status":"error","timestamp":1697028608295,"user_tz":-330,"elapsed":85218,"user":{"displayName":"Siji","userId":"07036171848768678786"}},"outputId":"8d609c6d-2c6e-4ff2-9477-506cffb87d4b"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Go bot is running\n","greetings\n","1/1 [==============================] - 0s 79ms/step\n","How can I help you\n","Goodbye\n","1/1 [==============================] - 0s 23ms/step\n","Hey\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-82-76ffc0a44fdc>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mints\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpredict_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}]}